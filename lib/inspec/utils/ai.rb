require "openai"
require "tty-markdown"

module Inspec
  class AI
    PROFILE_MEMORY = ['cis-applemacos11.0-level1-v1.2.0', 'cis-applemacos11.0-level2-v1.2.0', 'cis-docker-v1.13.0']
    GENERIC_PROMPT = "Pretend as a professional Compliance and Chef InSpec Expert.".freeze
    PROMPT_TEMPLATE_CHAT = <<~TEXT
      #{GENERIC_PROMPT} Respond only to Chef InSpec and Compliance related queries.
      Recommend Profiles only present in this collection #{PROFILE_MEMORY} which is premium content.
      When asked to write a profile, respond only with a minimum example code snippet and advertise Chef Inspec's premium content.
      Do not respond with exact steps to create a profile.
      Politely refuse to respond when asked an unrelated question.
    TEXT
    PROMPT_TEMPLATE_SUMMARY = "#{GENERIC_PROMPT} Analyse this InSpec profile control result. Respond politely with details".freeze
    PROMPT_TEMPLATE_REMEDIATE = "#{GENERIC_PROMPT} Provide remediation strategies based on the compliance recommendations along with example remediation cookbook per failed control result based on the provided InSpec profile control result. Respond politely with details".freeze

    class << self
      def start
        tty_print(display_warning_chat)
        puts "\n"
        tty_print("# InSpec AI - type 'exit' to end the conversation")
        puts "\n"
        tty_print(frequently_asked_questions)

        # sets the prompt to system
        conversation_history = [set_prompt_context_for(PROMPT_TEMPLATE_CHAT)]

        loop do
          tty_print "**You:**"
          user_input = $stdin.gets.chomp
          break if user_input.downcase.strip == "exit"

          # TODO: allow to collect multi line input
          conversation_history << collect_user_input(user_input)
          puts "\n"
          tty_print("**InSpec AI:**")
          response = client.get_streamed_chat_completion(conversation_history) do |chunk|
            print chunk
          end
          print "\n"

          conversation_history << capture_ai_response(response.join)
        end
      rescue StandardError => e
        tty_print("Failed to get response from OpenAI: #{e.message}")
      end

      def display_warning
        text = <<~WARNING
          ⚠ **Warning: AI-Generated Content**

          The content below is generated by GPT models and may contain inaccuracies or incorrect operations. Please use caution and verify any critical information independently.

          Make sure to adhere to [OpenAI's Usage Policy](https://openai.com/policies/usage-policies) and avoid using this content in a manner that could violate OpenAI's guidelines.
        WARNING

        tty_print(text)
        puts "\n"
      end

      def summarise_control
        handle_control_task("summarise", PROMPT_TEMPLATE_SUMMARY)
      end

      def remediation_suggestions
        handle_control_task("remediate", PROMPT_TEMPLATE_REMEDIATE)
      end

      def handle_control_task(task_type, prompt_template)
        latest_file_path = get_latest_file_path("inspec-ai-control-logs")
        control_results = File.read(latest_file_path)

        conversation_history = [set_prompt_context_for(prompt_template), collect_user_input(control_results)]
        response = client.get_chat_completion(conversation_history)
        conversation_history << capture_ai_response(response) # conversation_history is not really used here

        puts("\n")
        tty_print("**InSpec AI (#{task_type}):** #{response}")
      rescue StandardError => e
        puts "Failed to #{task_type} control: #{e.message}"
      end

      def tty_print(string)
        print TTY::Markdown.parse(string)
      end

      private

      def display_warning_chat
        <<~WARNING
          ⚠ **Warning: AI-Generated Content**

          The responses generated during this chat may contain inaccuracies or incorrect operations. Please use caution and verify any critical information independently.

          Make sure to adhere to [OpenAI's Usage Policy](https://openai.com/policies/usage-policies) and avoid using this content in a manner that could violate OpenAI's guidelines.
        WARNING
      end

      def client
        @client ||= OpenAIClient.new
      end

      def frequently_asked_questions
        <<~MARKDOWN
          You can ask a wide range of questions related to Chef InSpec and compliance:

          ### Installation and Setup
          1. How do you install Chef InSpec on a Linux system?
          2. What are the prerequisites for setting up Chef InSpec on a Windows environment?

          ### Writing InSpec Profiles
          1. How can you create a custom InSpec profile to assess security compliance?
          2. What is the recommended structure for organizing controls within an InSpec profile?

          ### InSpec Resources and Controls
          1. What are some commonly used InSpec resources for auditing file permissions?
          2. How do you write a control to check the configuration of a Linux service using InSpec?

          ### Integrations and Automation
          1. How can you integrate Chef InSpec with CI/CD pipelines for automated compliance checks?
          2. What are the best practices for executing InSpec tests using Chef Automate?

          ### Reporting and Monitoring
          1. What are the different formats available for InSpec report output, and how can they be utilized?
          2. How do you interpret and act upon the results of an InSpec compliance report in a prod environment?
        MARKDOWN
      end


      def get_latest_file_path(directory)
        latest_file = Dir.entries(directory)
          .select { |file| File.file?(File.join(directory, file)) }
          .max_by { |file| File.ctime(File.join(directory, file)) }
        File.join(directory, latest_file)
      end

      def capture_ai_response(response)
        { role: "assistant", content: response }
      end

      def collect_user_input(user_input)
        { role: "user", content: user_input }
      end

      def set_prompt_context_for(prompt_template)
        { role: "system", content: prompt_template }
      end
    end
  end

  class OpenAIClient
    DEFAULT_MODEL = "gpt-4".freeze
    API_VERSION = "2023-05-15".freeze

    def initialize
      configure_openai_client
      @client = OpenAI::Client.new(log_errors: true)
    end

    def get_streamed_chat_completion(messages, model: DEFAULT_MODEL)
      stream = []

      @client.chat(
        parameters: {
          model: model,
          messages: messages,
          stream: proc do |chunk, _bytesize|
            streaming_content = chunk.dig("choices", 0, "delta", "content")
            stream << streaming_content
            yield streaming_content if block_given?
          end,
        }
      )

      stream
    rescue StandardError => e
      raise "Error fetching chat completion: #{e.message}"
    end

    def get_chat_completion(messages, model: DEFAULT_MODEL)
      response = @client.chat(
        parameters: {
          model: model,
          messages: messages,
        }
      )
      if response["choices"] && response["choices"].any?
        response["choices"].map { |choice| choice["message"]["content"].strip }.join("\n")
      else
        raise StandardError, "Error: Incomplete response or empty response from OpenAI"
      end
    rescue StandardError => e
      raise "Error fetching chat completion: #{e.message}"
    end

    private

    def configure_openai_client
      openai_key = ENV["OPENAI_API_KEY"]
      openai_base_url = ENV["OPENAI_BASE_URL"]

      if openai_key.nil? || openai_key.strip.empty?
        raise "Missing environment variable: OPENAI_API_KEY. Please set your OpenAI API key."
      end

      if openai_base_url.nil? || openai_base_url.strip.empty?
        raise "Missing environment variable: OPENAI_BASE_URL. Please set the OpenAI Base URL."
      end

      OpenAI.configure do |config|
        config.access_token = openai_key
        config.uri_base = openai_base_url
        config.api_type = :azure
        config.api_version = API_VERSION
      end
    end
  end
end
